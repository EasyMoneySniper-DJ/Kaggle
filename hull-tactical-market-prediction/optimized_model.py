import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.cluster import FeatureAgglomeration
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.pipeline import Pipeline
from pykalman import KalmanFilter
import matplotlib.pyplot as plt
import warnings

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼ˆè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # Macç³»ç»Ÿ
# plt.rcParams['font.sans-serif'] = ['SimHei']  # Windowsç³»ç»Ÿ
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# å¿½ç•¥ SettingWithCopyWarning
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)

# ============================================
# 1. æ•°æ®åŠ è½½ä¸åŸºç¡€å‡†å¤‡
# ============================================
df = pd.read_csv('train.csv')

# å®šä¹‰ç‰¹å¾å’Œç›®æ ‡
target_col = 'market_forward_excess_returns'
ignore_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']
feature_cols = [c for c in df.columns if c not in ignore_cols]

# å¡«å……ç¼ºå¤±å€¼
X_all = df[feature_cols].ffill().fillna(0)
y_all = df[target_col]

# æ—¶é—´åºåˆ—åˆ‡åˆ† (80/20)
split = int(len(df) * 0.8)
X_train, X_val = X_all.iloc[:split].copy(), X_all.iloc[split:].copy()  # ä½¿ç”¨ .copy() é¿å…è­¦å‘Š
y_train, y_val = y_all.iloc[:split], y_all.iloc[split:]

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}, éªŒè¯é›†å¤§å°: {len(X_val)}")

# ============================================
# 2. åˆ›å»ºæ»åç‰¹å¾ + æ—¶åºç»Ÿè®¡ç‰¹å¾ï¼ˆè§£å†³è¿‡æ‹Ÿåˆï¼‰
# ============================================
print("\nã€æ­¥éª¤1ã€‘åˆ›å»ºæ»åç‰¹å¾å’Œæ—¶åºç»Ÿè®¡ç‰¹å¾...")

# åŸºç¡€æ»åç‰¹å¾
df["lagged_forward_returns"] = df["forward_returns"].shift(1)
df["lagged_risk_free_rate"] = df["risk_free_rate"].shift(1)
df["lagged_market_forward_excess_returns"] = df["market_forward_excess_returns"].shift(1)

# å¡«å……ç¬¬ä¸€è¡Œçš„NaN
df["lagged_forward_returns"] = df["lagged_forward_returns"].fillna(0)
df["lagged_risk_free_rate"] = df["lagged_risk_free_rate"].fillna(0)
df["lagged_market_forward_excess_returns"] = df["lagged_market_forward_excess_returns"].fillna(0)

# ============================================
# æ–°å¢ï¼šæ—¶åºç»Ÿè®¡ç‰¹å¾ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
# ============================================
target_series = df['market_forward_excess_returns']

# 1. æ»šåŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
for window in [5, 10, 20]:
    df[f'rolling_mean_{window}'] = target_series.rolling(window, min_periods=1).mean()
    df[f'rolling_std_{window}'] = target_series.rolling(window, min_periods=1).std().fillna(0)
    df[f'rolling_max_{window}'] = target_series.rolling(window, min_periods=1).max()
    df[f'rolling_min_{window}'] = target_series.rolling(window, min_periods=1).min()

# 2. åŠ¨é‡ç‰¹å¾ï¼ˆmomentumï¼‰
for period in [5, 10]:
    df[f'momentum_{period}'] = target_series.rolling(period, min_periods=1).sum()

# 3. æ³¢åŠ¨ç‡ç‰¹å¾ï¼ˆvolatilityï¼‰
df['volatility_5'] = target_series.rolling(5, min_periods=1).std().fillna(0)
df['volatility_20'] = target_series.rolling(20, min_periods=1).std().fillna(0)

# 4. è¶‹åŠ¿ç‰¹å¾ï¼ˆæ˜¯å¦ä¸Šæ¶¨ï¼‰
df['trend_5'] = (target_series.rolling(5, min_periods=1).mean() > 0).astype(int)
df['trend_10'] = (target_series.rolling(10, min_periods=1).mean() > 0).astype(int)

# 5. æ»åå·®åˆ†ç‰¹å¾
df['return_diff_1'] = target_series.diff(1).fillna(0)
df['return_diff_5'] = target_series.diff(5).fillna(0)

# æ›´æ–°ç‰¹å¾åˆ—è¡¨
new_features = [
    'lagged_forward_returns', 'lagged_risk_free_rate', 'lagged_market_forward_excess_returns',
    'rolling_mean_5', 'rolling_mean_10', 'rolling_mean_20',
    'rolling_std_5', 'rolling_std_10', 'rolling_std_20',
    'rolling_max_5', 'rolling_max_10', 'rolling_max_20',
    'rolling_min_5', 'rolling_min_10', 'rolling_min_20',
    'momentum_5', 'momentum_10',
    'volatility_5', 'volatility_20',
    'trend_5', 'trend_10',
    'return_diff_1', 'return_diff_5'
]
feature_cols.extend(new_features)

# é‡æ–°æå–ç‰¹å¾
X_all = df[feature_cols].ffill().fillna(0)

# é‡æ–°åˆ‡åˆ†
X_train, X_val = X_all.iloc[:split].copy(), X_all.iloc[split:].copy()

print(f"âœ… åˆ›å»ºäº† {len(new_features)} ä¸ªæ–°ç‰¹å¾ï¼ˆåŒ…æ‹¬æ»åã€æ»šåŠ¨ã€åŠ¨é‡ã€æ³¢åŠ¨ç‡ç­‰ï¼‰")
print(f"   å½“å‰ç‰¹å¾æ€»æ•°: {len(feature_cols)}")

# ============================================
# 3. å¡å°”æ›¼æ»¤æ³¢ç‰¹å¾å·¥ç¨‹
# ============================================
print("\nã€æ­¥éª¤2ã€‘åº”ç”¨å¡å°”æ›¼æ»¤æ³¢...")

# é€‰æ‹©ç”¨äºå¡å°”æ›¼æ»¤æ³¢çš„è§‚æµ‹ç‰¹å¾
obs_features = ['lagged_forward_returns', 'lagged_market_forward_excess_returns']

# ä»dfä¸­æå–è§‚æµ‹æ•°æ®
obs_data_train = df[obs_features].iloc[:split].values
obs_data_val = df[obs_features].iloc[split:].values

# åˆå§‹åŒ–å¡å°”æ›¼æ»¤æ³¢å™¨
kf = KalmanFilter(
    n_dim_obs=len(obs_features),  # è§‚æµ‹ç»´åº¦ = 2
    n_dim_state=5  # éšçŠ¶æ€ç»´åº¦ = 5
)

# EMç®—æ³•è®­ç»ƒå¡å°”æ›¼æ»¤æ³¢å™¨
print("  è®­ç»ƒå¡å°”æ›¼æ»¤æ³¢å™¨...")
kf = kf.em(obs_data_train, n_iter=5)

# å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œæ»¤æ³¢
print("  åº”ç”¨å¡å°”æ›¼æ»¤æ³¢...")
filtered_train, _ = kf.filter(obs_data_train)
filtered_val, _ = kf.filter(obs_data_val)

# å°†æ»¤æ³¢åçš„çŠ¶æ€ä½œä¸ºæ–°ç‰¹å¾
for i in range(filtered_train.shape[1]):
    X_train[f'KF_state_{i}'] = filtered_train[:, i]
    X_val[f'KF_state_{i}'] = filtered_val[:, i]

# æ›´æ–°ç‰¹å¾åˆ—è¡¨
feature_cols_with_kf = list(X_train.columns)
print(f"âœ… æ·»åŠ äº† {filtered_train.shape[1]} ä¸ªå¡å°”æ›¼æ»¤æ³¢ç‰¹å¾")
print(f"   æœ€ç»ˆç‰¹å¾æ€»æ•°: {len(feature_cols_with_kf)}")

# ============================================
# 4. ç½‘æ ¼æœç´¢æœ€ä¼˜èšç±»æ•°
# ============================================
print("\nã€æ­¥éª¤3ã€‘ç½‘æ ¼æœç´¢æœ€ä¼˜èšç±»æ•°...")


def calculate_ic(y_true, y_pred):
    """è®¡ç®— Information Coefficient (IC)"""
    return np.corrcoef(y_pred, y_true)[0, 1]


def train_and_evaluate(X_tr, y_tr, X_vl, y_vl):
    """è®­ç»ƒLGBMå¹¶è¿”å›IC"""
    # é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'learning_rate': 0.03,  # é™ä½å­¦ä¹ ç‡ï¼š0.05 â†’ 0.03
        'num_leaves': 15,  # å‡å°‘å¶å­æ•°ï¼š31 â†’ 15
        'max_depth': 4,  # é™åˆ¶æ·±åº¦ï¼š6 â†’ 4
        'feature_fraction': 0.7,  # æ¯æ¬¡åªç”¨70%ç‰¹å¾ï¼š0.8 â†’ 0.7
        'bagging_fraction': 0.7,  # æ¯æ¬¡åªç”¨70%æ•°æ®ï¼š0.7
        'bagging_freq': 5,
        'reg_alpha': 0.3,  # å¢å¼ºL1æ­£åˆ™åŒ–ï¼š0.1 â†’ 0.3
        'reg_lambda': 0.3,  # å¢å¼ºL2æ­£åˆ™åŒ–ï¼š0.1 â†’ 0.3
        'min_child_samples': 30,  # å¢åŠ æœ€å°æ ·æœ¬æ•°ï¼š20 â†’ 30
        'verbose': -1,
        'seed': 42
    }

    dtrain = lgb.Dataset(X_tr, label=y_tr)
    dval = lgb.Dataset(X_vl, label=y_vl, reference=dtrain)

    model = lgb.train(
        params,
        dtrain,
        valid_sets=[dval],
        num_boost_round=200,  # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼š300 â†’ 200
        callbacks=[
            lgb.early_stopping(stopping_rounds=30),  # æ›´æ—©åœæ­¢ï¼š50 â†’ 30
            lgb.log_evaluation(0)
        ]
    )

    preds = model.predict(X_vl)
    ic = calculate_ic(y_vl, preds)
    return ic, model


# æµ‹è¯•ä¸åŒçš„èšç±»æ•°
cluster_nums = [10, 15, 20, 25, 30]
cluster_results = {}

for n_clusters in cluster_nums:
    print(f"\n  æµ‹è¯• n_clusters = {n_clusters}...")

    # æ„å»ºpipeline
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('cluster', FeatureAgglomeration(n_clusters=n_clusters))
    ])

    # è½¬æ¢ç‰¹å¾
    X_train_trans = pipe.fit_transform(X_train)
    X_val_trans = pipe.transform(X_val)

    # è®­ç»ƒè¯„ä¼°
    ic, _ = train_and_evaluate(X_train_trans, y_train, X_val_trans, y_val)
    cluster_results[n_clusters] = ic
    print(f"  âœ… IC = {ic:.4f}")

# æ‰¾åˆ°æœ€ä¼˜èšç±»æ•°
best_n_clusters = max(cluster_results, key=cluster_results.get)
best_ic = cluster_results[best_n_clusters]
print(f"\nğŸ† æœ€ä¼˜èšç±»æ•°: {best_n_clusters}, IC = {best_ic:.4f}")

# å¯è§†åŒ–ï¼ˆä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸ï¼‰
plt.figure(figsize=(10, 6))
plt.plot(list(cluster_results.keys()), list(cluster_results.values()),
         marker='o', linewidth=2, markersize=8)
plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
plt.xlabel('Number of Clusters', fontsize=12)
plt.ylabel('Validation IC', fontsize=12)
plt.title('èšç±»æ•°é‡ vs æ¨¡å‹è¡¨ç°', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('cluster_optimization.png', dpi=300, bbox_inches='tight')
print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜: cluster_optimization.png")

# ============================================
# 5. æ—¶åºäº¤å‰éªŒè¯
# ============================================
print(f"\nã€æ­¥éª¤4ã€‘ä½¿ç”¨æœ€ä¼˜èšç±»æ•° ({best_n_clusters}) è¿›è¡Œæ—¶åºäº¤å‰éªŒè¯...")

# ä½¿ç”¨æœ€ä¼˜èšç±»æ•°çš„pipeline
optimal_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('cluster', FeatureAgglomeration(n_clusters=best_n_clusters))
])

# TimeSeriesSplitäº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=5)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):
    print(f"\n  Fold {fold}/5...")

    # åˆ†å‰²æ•°æ®
    X_tr = X_train.iloc[train_idx]
    X_vl = X_train.iloc[val_idx]
    y_tr = y_train.iloc[train_idx]
    y_vl = y_train.iloc[val_idx]

    # ç‰¹å¾è½¬æ¢
    X_tr_trans = optimal_pipe.fit_transform(X_tr)
    X_vl_trans = optimal_pipe.transform(X_vl)

    # è®­ç»ƒè¯„ä¼°
    ic, _ = train_and_evaluate(X_tr_trans, y_tr, X_vl_trans, y_vl)
    cv_scores.append(ic)
    print(f"  IC = {ic:.4f}")

print(f"\nğŸ“ˆ äº¤å‰éªŒè¯ç»“æœ:")
print(f"  å¹³å‡ IC: {np.mean(cv_scores):.4f}")
print(f"  æ ‡å‡†å·®:   {np.std(cv_scores):.4f}")
print(f"  å„æŠ˜ IC: {[f'{x:.4f}' for x in cv_scores]}")

# ============================================
# 6. æœ€ç»ˆæ¨¡å‹è®­ç»ƒ
# ============================================
print("\nã€æ­¥éª¤5ã€‘è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")

# ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
X_train_final = optimal_pipe.fit_transform(X_train)
X_val_final = optimal_pipe.transform(X_val)

final_ic, final_model = train_and_evaluate(X_train_final, y_train, X_val_final, y_val)

print(f"\nğŸ¯ æœ€ç»ˆéªŒè¯é›† IC: {final_ic:.4f}")

# ============================================
# 7. ç‰¹å¾é‡è¦æ€§åˆ†æ
# ============================================
print("\nã€æ­¥éª¤6ã€‘åˆ†æç‰¹å¾é‡è¦æ€§...")

feature_importance = pd.DataFrame({
    'feature': [f'Cluster_{i}' for i in range(best_n_clusters)],
    'importance': final_model.feature_importance(importance_type='gain')
}).sort_values('importance', ascending=False)

print("\nTop 10 é‡è¦ç‰¹å¾:")
print(feature_importance.head(10).to_string(index=False))

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(10, 6))
top_features = feature_importance.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Feature Importance (Gain)', fontsize=12)
plt.title('Top 15 ç‰¹å¾é‡è¦æ€§', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜: feature_importance.png")

# ============================================
# 8. é¢„æµ‹ä¸è¯„ä¼°
# ============================================
print("\nã€æ­¥éª¤7ã€‘æœ€ç»ˆé¢„æµ‹ä¸è¯„ä¼°...")

y_pred = final_model.predict(X_val_final)

# è®¡ç®—é¢å¤–è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import mean_squared_error, mean_absolute_error

rmse = np.sqrt(mean_squared_error(y_val, y_pred))
mae = mean_absolute_error(y_val, y_pred)

print(f"\nğŸ“Š æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡:")
print(f"  IC (Information Coefficient): {final_ic:.4f}")
print(f"  RMSE: {rmse:.6f}")
print(f"  MAE:  {mae:.6f}")

# é¢„æµ‹ vs å®é™…å€¼å¯è§†åŒ–
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_val, y_pred, alpha=0.5, s=10)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Actual Returns', fontsize=12)
plt.ylabel('Predicted Returns', fontsize=12)
plt.title(f'é¢„æµ‹ vs å®é™… (IC={final_ic:.4f})', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
residuals = y_val - y_pred
plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel('Residuals', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
plt.axvline(x=0, color='r', linestyle='--', lw=2)
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')
print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜: prediction_analysis.png")

print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
print("\n" + "=" * 60)
print("ğŸ“Š æ”¹è¿›æ•ˆæœå¯¹æ¯”:")
print("=" * 60)
print(f"  åŸå§‹ Baseline:              IC = -0.0449")
print(f"  ä½ çš„ Clustering (20):        IC = +0.0471")
print(f"  ä¼˜åŒ–å (KF + æ—¶åºç‰¹å¾ + {best_n_clusters}): IC = {final_ic:.4f}")
print(f"  æå‡å¹…åº¦:                    {((final_ic - 0.0471) / 0.0471 * 100):.1f}%")
print("=" * 60)
print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
print("  1. âœ… å·²è§£å†³ï¼šmatplotlibä¸­æ–‡æ˜¾ç¤ºé—®é¢˜")
print("  2. âœ… å·²è§£å†³ï¼šé€šè¿‡å¢åŠ æ—¶åºç‰¹å¾å’Œé™ä½æ¨¡å‹å¤æ‚åº¦å‡å°‘è¿‡æ‹Ÿåˆ")
print("  3. ğŸ“Œ ç»§ç»­ä¼˜åŒ–ï¼šè€ƒè™‘æ¨¡å‹èåˆï¼ˆLGBM + CatBoost + XGBoostï¼‰")
print("  4. ğŸ“Œ å¯é€‰ï¼šè¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼ˆRandomizedSearchCVï¼‰")